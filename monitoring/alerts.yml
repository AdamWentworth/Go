groups:
  - name: auth-service-alerts
    rules:
      - alert: AuthServiceTargetDown
        expr: up{job="auth_service"} == 0
        for: 2m
        labels:
          severity: critical
          service: auth_service
        annotations:
          summary: "auth_service target is down"
          description: "Prometheus cannot scrape auth_service for at least 2 minutes."

      - alert: AuthServiceHigh5xxRate
        expr: |
          (
            sum(rate(http_requests_total{job="auth_service",status=~"5.."}[5m]))
            /
            clamp_min(sum(rate(http_requests_total{job="auth_service"}[5m])), 1)
          ) > 0.02
        for: 10m
        labels:
          severity: warning
          service: auth_service
        annotations:
          summary: "auth_service 5xx rate is high"
          description: "5xx responses exceed 2% for 10 minutes."

      - alert: AuthServiceHighP95Latency
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job="auth_service"}[5m])
            )
          ) > 1
        for: 10m
        labels:
          severity: warning
          service: auth_service
        annotations:
          summary: "auth_service p95 latency is high"
          description: "p95 latency exceeds 1 second for 10 minutes."

      - alert: AuthServiceMetricsMissing
        expr: absent(http_requests_total{job="auth_service"})
        for: 10m
        labels:
          severity: warning
          service: auth_service
        annotations:
          summary: "auth_service request metrics are missing"
          description: "http_requests_total has been absent for 10 minutes."

  - name: receiver-service-alerts
    rules:
      - alert: ReceiverServiceTargetDown
        expr: up{job="receiver_service"} == 0
        for: 2m
        labels:
          severity: critical
          service: receiver_service
        annotations:
          summary: "receiver_service target is down"
          description: "Prometheus cannot scrape receiver_service for at least 2 minutes."

      - alert: ReceiverServiceHigh5xxRate
        expr: |
          (
            sum(rate(http_requests_total{job="receiver_service",status=~"5.."}[5m]))
            /
            clamp_min(sum(rate(http_requests_total{job="receiver_service"}[5m])), 1)
          ) > 0.02
        for: 10m
        labels:
          severity: warning
          service: receiver_service
        annotations:
          summary: "receiver_service 5xx rate is high"
          description: "5xx responses exceed 2% for 10 minutes."

      - alert: ReceiverServiceHighP95Latency
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job="receiver_service"}[5m])
            )
          ) > 1
        for: 10m
        labels:
          severity: warning
          service: receiver_service
        annotations:
          summary: "receiver_service p95 latency is high"
          description: "p95 latency exceeds 1 second for 10 minutes."

      - alert: ReceiverServiceMetricsMissing
        expr: absent(http_requests_total{job="receiver_service"})
        for: 10m
        labels:
          severity: warning
          service: receiver_service
        annotations:
          summary: "receiver_service request metrics are missing"
          description: "http_requests_total has been absent for 10 minutes."

  - name: storage-service-alerts
    rules:
      - alert: StorageServiceTargetDown
        expr: up{job="storage_service"} == 0
        for: 2m
        labels:
          severity: critical
          service: storage_service
        annotations:
          summary: "storage_service target is down"
          description: "Prometheus cannot scrape storage_service for at least 2 minutes."

      - alert: StorageServiceMetricsMissing
        expr: absent(http_requests_total{job="storage_service"})
        for: 10m
        labels:
          severity: warning
          service: storage_service
        annotations:
          summary: "storage_service request metrics are missing"
          description: "http_requests_total has been absent for 10 minutes."

      - alert: StorageKafkaConsumerNotReady
        expr: storage_kafka_consumer_ready{job="storage_service"} == 0
        for: 2m
        labels:
          severity: warning
          service: storage_service
        annotations:
          summary: "storage_service Kafka consumer is not ready"
          description: "storage_kafka_consumer_ready has been 0 for at least 2 minutes."

  - name: events-service-alerts
    rules:
      - alert: EventsServiceTargetDown
        expr: up{job="events_service"} == 0
        for: 2m
        labels:
          severity: critical
          service: events_service
        annotations:
          summary: "events_service target is down"
          description: "Prometheus cannot scrape events_service for at least 2 minutes."

      - alert: EventsServiceHigh5xxRate
        expr: |
          (
            sum(rate(http_requests_total{job="events_service",status=~"5.."}[5m]))
            /
            clamp_min(sum(rate(http_requests_total{job="events_service"}[5m])), 1)
          ) > 0.02
        for: 10m
        labels:
          severity: warning
          service: events_service
        annotations:
          summary: "events_service 5xx rate is high"
          description: "5xx responses exceed 2% for 10 minutes."

      - alert: EventsServiceHighP95Latency
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job="events_service"}[5m])
            )
          ) > 1
        for: 10m
        labels:
          severity: warning
          service: events_service
        annotations:
          summary: "events_service p95 latency is high"
          description: "p95 latency exceeds 1 second for 10 minutes."

      - alert: EventsServiceMetricsMissing
        expr: absent(http_requests_total{job="events_service"})
        for: 10m
        labels:
          severity: warning
          service: events_service
        annotations:
          summary: "events_service request metrics are missing"
          description: "http_requests_total has been absent for 10 minutes."

  - name: users-service-alerts
    rules:
      - alert: UsersServiceTargetDown
        expr: up{job="users_service"} == 0
        for: 2m
        labels:
          severity: critical
          service: users_service
        annotations:
          summary: "users_service target is down"
          description: "Prometheus cannot scrape users_service for at least 2 minutes."

      - alert: UsersServiceHigh5xxRate
        expr: |
          (
            sum(rate(http_requests_total{job="users_service",status=~"5.."}[5m]))
            /
            clamp_min(sum(rate(http_requests_total{job="users_service"}[5m])), 1)
          ) > 0.02
        for: 10m
        labels:
          severity: warning
          service: users_service
        annotations:
          summary: "users_service 5xx rate is high"
          description: "5xx responses exceed 2% for 10 minutes."

      - alert: UsersServiceHighP95Latency
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job="users_service"}[5m])
            )
          ) > 1
        for: 10m
        labels:
          severity: warning
          service: users_service
        annotations:
          summary: "users_service p95 latency is high"
          description: "p95 latency exceeds 1 second for 10 minutes."

      - alert: UsersServiceMetricsMissing
        expr: absent(http_requests_total{job="users_service"})
        for: 10m
        labels:
          severity: warning
          service: users_service
        annotations:
          summary: "users_service request metrics are missing"
          description: "http_requests_total has been absent for 10 minutes."

  - name: search-service-alerts
    rules:
      - alert: SearchServiceTargetDown
        expr: up{job="search_service"} == 0
        for: 2m
        labels:
          severity: critical
          service: search_service
        annotations:
          summary: "search_service target is down"
          description: "Prometheus cannot scrape search_service for at least 2 minutes."

      - alert: SearchServiceHigh5xxRate
        expr: |
          (
            sum(rate(http_requests_total{job="search_service",status=~"5.."}[5m]))
            /
            clamp_min(sum(rate(http_requests_total{job="search_service"}[5m])), 1)
          ) > 0.02
        for: 10m
        labels:
          severity: warning
          service: search_service
        annotations:
          summary: "search_service 5xx rate is high"
          description: "5xx responses exceed 2% for 10 minutes."

      - alert: SearchServiceHighP95Latency
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job="search_service"}[5m])
            )
          ) > 1
        for: 10m
        labels:
          severity: warning
          service: search_service
        annotations:
          summary: "search_service p95 latency is high"
          description: "p95 latency exceeds 1 second for 10 minutes."

      - alert: SearchServiceMetricsMissing
        expr: absent(http_requests_total{job="search_service"})
        for: 10m
        labels:
          severity: warning
          service: search_service
        annotations:
          summary: "search_service request metrics are missing"
          description: "http_requests_total has been absent for 10 minutes."

  - name: location-service-alerts
    rules:
      - alert: LocationServiceTargetDown
        expr: up{job="location_service"} == 0
        for: 2m
        labels:
          severity: critical
          service: location_service
        annotations:
          summary: "location_service target is down"
          description: "Prometheus cannot scrape location_service for at least 2 minutes."

      - alert: LocationServiceHigh5xxRate
        expr: |
          (
            sum(rate(http_requests_total{job="location_service",status=~"5.."}[5m]))
            /
            clamp_min(sum(rate(http_requests_total{job="location_service"}[5m])), 1)
          ) > 0.02
        for: 10m
        labels:
          severity: warning
          service: location_service
        annotations:
          summary: "location_service 5xx rate is high"
          description: "5xx responses exceed 2% for 10 minutes."

      - alert: LocationServiceHighP95Latency
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job="location_service"}[5m])
            )
          ) > 1
        for: 10m
        labels:
          severity: warning
          service: location_service
        annotations:
          summary: "location_service p95 latency is high"
          description: "p95 latency exceeds 1 second for 10 minutes."

      - alert: LocationServiceMetricsMissing
        expr: absent(http_requests_total{job="location_service"})
        for: 10m
        labels:
          severity: warning
          service: location_service
        annotations:
          summary: "location_service request metrics are missing"
          description: "http_requests_total has been absent for 10 minutes."

  - name: frontend-probe-alerts
    rules:
      - alert: FrontendProbeDown
        expr: probe_success{job="frontend_probe"} == 0
        for: 2m
        labels:
          severity: critical
          service: frontend
        annotations:
          summary: "frontend probe is down"
          description: "Blackbox probe to https://pokemongonexus.com/ is failing for at least 2 minutes."

      - alert: FrontendProbeHighLatency
        expr: probe_duration_seconds{job="frontend_probe"} > 1.5
        for: 10m
        labels:
          severity: warning
          service: frontend
        annotations:
          summary: "frontend probe latency is high"
          description: "Probe duration is above 1.5 seconds for at least 10 minutes."

      - alert: FrontendTLSCertExpiringSoon
        expr: (probe_ssl_earliest_cert_expiry{job="frontend_probe"} - time()) < 1209600
        for: 1h
        labels:
          severity: warning
          service: frontend
        annotations:
          summary: "frontend TLS certificate expires soon"
          description: "Certificate for pokemongonexus.com expires in under 14 days."

  - name: pokemon-data-alerts
    rules:
      - alert: PokemonDataTargetDown
        expr: up{job="pokemon_data"} == 0
        for: 2m
        labels:
          severity: critical
          service: pokemon_data
        annotations:
          summary: "pokemon_data target is down"
          description: "Prometheus cannot scrape pokemon_data for at least 2 minutes."

      - alert: PokemonDataHigh5xxRate
        expr: |
          (
            sum(rate(http_requests_total{job="pokemon_data",status=~"5.."}[5m]))
            /
            clamp_min(sum(rate(http_requests_total{job="pokemon_data"}[5m])), 1)
          ) > 0.02
        for: 10m
        labels:
          severity: warning
          service: pokemon_data
        annotations:
          summary: "pokemon_data 5xx rate is high"
          description: "5xx responses exceed 2% for 10 minutes."

      - alert: PokemonDataHighP95Latency
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job="pokemon_data",route="/pokemon/pokemons"}[5m])
            )
          ) > 1
        for: 10m
        labels:
          severity: warning
          service: pokemon_data
        annotations:
          summary: "pokemon_data /pokemon/pokemons p95 latency is high"
          description: "p95 latency for /pokemon/pokemons exceeds 1 second for 10 minutes."

      - alert: PokemonDataHighRateLimitedRequests
        expr: sum(rate(http_requests_total{job="pokemon_data",route="/pokemon/pokemons",status="429"}[5m])) > 2
        for: 10m
        labels:
          severity: warning
          service: pokemon_data
        annotations:
          summary: "pokemon_data is rate limiting heavily"
          description: "429 responses for /pokemon/pokemons exceed 2 req/s over 10 minutes."

      - alert: PokemonDataMetricsMissing
        expr: absent(http_requests_total{job="pokemon_data"})
        for: 10m
        labels:
          severity: warning
          service: pokemon_data
        annotations:
          summary: "pokemon_data request metrics are missing"
          description: "http_requests_total has been absent for 10 minutes."

  - name: kafka-alerts
    rules:
      - alert: KafkaExporterTargetDown
        expr: up{job="kafka_exporter"} == 0
        for: 2m
        labels:
          severity: critical
          service: kafka
        annotations:
          summary: "kafka_exporter target is down"
          description: "Prometheus cannot scrape kafka_exporter for at least 2 minutes."

      - alert: KafkaBrokerUnavailable
        expr: kafka_brokers < 1
        for: 2m
        labels:
          severity: critical
          service: kafka
        annotations:
          summary: "Kafka broker is unavailable"
          description: "kafka_exporter reports fewer than 1 active broker."

      - alert: KafkaTopicMissingBatchedUpdates
        expr: absent(kafka_topic_partitions{topic="batchedUpdates"})
        for: 5m
        labels:
          severity: warning
          service: kafka
        annotations:
          summary: "Kafka topic batchedUpdates is missing"
          description: "kafka_exporter cannot find the batchedUpdates topic for at least 5 minutes."

      - alert: KafkaConsumerLagHigh
        expr: max(kafka_consumergroup_lag_sum{consumergroup=~"event_group|sse_consumer_group"}) > 1000
        for: 10m
        labels:
          severity: warning
          service: kafka
        annotations:
          summary: "Kafka consumer lag is high"
          description: "Kafka consumer lag exceeded 1000 messages for 10 minutes."

  - name: host-and-container-alerts
    rules:
      - alert: HostHighCPU
        expr: (100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 85
        for: 10m
        labels:
          severity: warning
          service: host
        annotations:
          summary: "Host CPU usage is high"
          description: "CPU usage is above 85% for at least 10 minutes."

      - alert: HostHighMemory
        expr: ((1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100) > 90
        for: 10m
        labels:
          severity: warning
          service: host
        annotations:
          summary: "Host memory usage is high"
          description: "Memory usage is above 90% for at least 10 minutes."

      - alert: HostLowDisk
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"}) * 100 < 10
        for: 15m
        labels:
          severity: warning
          service: host
        annotations:
          summary: "Host disk space is low"
          description: "A filesystem has less than 10% free space for at least 15 minutes."

      - alert: ContainerHighCPU
        expr: sum by (name) (rate(container_cpu_usage_seconds_total{name!=""}[5m])) > 0.8
        for: 10m
        labels:
          severity: warning
          service: containers
        annotations:
          summary: "Container CPU usage is high"
          description: "Container {{ $labels.name }} is using more than 0.8 CPU cores for at least 10 minutes."

      - alert: ContainerHighMemory
        expr: (container_memory_working_set_bytes{name!="",image!=""} / 1024 / 1024) > 1024
        for: 10m
        labels:
          severity: warning
          service: containers
        annotations:
          summary: "Container memory usage is high"
          description: "Container {{ $labels.name }} is using more than 1 GiB memory for at least 10 minutes."

