name: deploy-kafka-prod

on:
  workflow_dispatch:
    inputs:
      deploy_root:
        description: "Absolute path to the Go repo on prod runner"
        required: false
        default: "/media/adam/storage/Code/Go"
        type: string

permissions:
  contents: read

jobs:
  deploy:
    name: Deploy Kafka To Prod
    runs-on: [self-hosted, linux, x64, prod]
    environment: production
    concurrency:
      group: deploy-kafka-prod
      cancel-in-progress: false

    steps:
      - name: Sync Deploy Repo
        shell: bash
        run: |
          set -euo pipefail

          DEPLOY_ROOT="${{ inputs.deploy_root }}"
          DEPLOY_REF="${{ github.ref_name }}"

          if [[ ! -d "${DEPLOY_ROOT}/.git" ]]; then
            echo "Deploy root is not a git repo: ${DEPLOY_ROOT}" >&2
            exit 1
          fi

          if ! git -C "${DEPLOY_ROOT}" diff --quiet || ! git -C "${DEPLOY_ROOT}" diff --cached --quiet; then
            echo "Deploy root has local tracked changes: ${DEPLOY_ROOT}" >&2
            git -C "${DEPLOY_ROOT}" status --short || true
            echo "Commit/stash/revert tracked changes before using automated deploy." >&2
            exit 1
          fi

          UNTRACKED_COUNT="$(git -C "${DEPLOY_ROOT}" ls-files --others --exclude-standard | wc -l | tr -d ' ')"
          if [[ "${UNTRACKED_COUNT}" != "0" ]]; then
            echo "Warning: ${UNTRACKED_COUNT} untracked files present in deploy root; continuing."
          fi

          echo "Syncing ${DEPLOY_ROOT} to origin/${DEPLOY_REF}"
          git -C "${DEPLOY_ROOT}" fetch --prune origin
          git -C "${DEPLOY_ROOT}" checkout "${DEPLOY_REF}"
          git -C "${DEPLOY_ROOT}" pull --ff-only origin "${DEPLOY_REF}"

      - name: Preflight Checks
        shell: bash
        run: |
          set -euo pipefail

          DEPLOY_ROOT="${{ inputs.deploy_root }}"
          COMPOSE_FILE="${DEPLOY_ROOT}/kafka/docker-compose.yml"
          KAFKA_NETWORK="kafka_default"
          ZK_DATA_DIR="${DEPLOY_ROOT}/kafka/data/confluent/zookeeper/data"
          ZK_LOG_DIR="${DEPLOY_ROOT}/kafka/data/confluent/zookeeper/log"
          KAFKA_DATA_DIR="${DEPLOY_ROOT}/kafka/data/confluent/kafka"

          if [[ ! -d "${DEPLOY_ROOT}" ]]; then
            echo "Deploy root not found: ${DEPLOY_ROOT}" >&2
            exit 1
          fi

          if [[ ! -f "${COMPOSE_FILE}" ]]; then
            echo "Compose file not found: ${COMPOSE_FILE}" >&2
            exit 1
          fi

          if ! docker network inspect "${KAFKA_NETWORK}" >/dev/null 2>&1; then
            echo "External docker network ${KAFKA_NETWORK} not found; creating it now."
            docker network create "${KAFKA_NETWORK}"
          fi

          mkdir -p "${ZK_DATA_DIR}" "${ZK_LOG_DIR}" "${KAFKA_DATA_DIR}"
          docker run --rm \
            -v "${ZK_DATA_DIR}:/zk_data" \
            -v "${ZK_LOG_DIR}:/zk_log" \
            -v "${KAFKA_DATA_DIR}:/kafka_data" \
            alpine:3.20 sh -ec '
              chown -R 1000:1000 /zk_data /zk_log /kafka_data || true
              find /zk_data /zk_log /kafka_data -type d -exec chmod 775 {} +
              find /zk_data /zk_log /kafka_data -type f -exec chmod 664 {} +
            '

          if ! docker run --rm --user 1000:1000 -v "${ZK_DATA_DIR}:/zk_data" alpine:3.20 sh -ec 'touch /zk_data/.rw_test && rm -f /zk_data/.rw_test'; then
            echo "Zookeeper data dir is not writable by UID 1000: ${ZK_DATA_DIR}" >&2
            exit 1
          fi
          if ! docker run --rm --user 1000:1000 -v "${KAFKA_DATA_DIR}:/kafka_data" alpine:3.20 sh -ec 'touch /kafka_data/.rw_test && rm -f /kafka_data/.rw_test'; then
            echo "Kafka data dir is not writable by UID 1000: ${KAFKA_DATA_DIR}" >&2
            exit 1
          fi

          docker compose -f "${COMPOSE_FILE}" config >/dev/null

      - name: Deploy Kafka Stack
        shell: bash
        run: |
          set -euo pipefail

          DEPLOY_ROOT="${{ inputs.deploy_root }}"
          COMPOSE_FILE="${DEPLOY_ROOT}/kafka/docker-compose.yml"

          if ! docker compose -f "${COMPOSE_FILE}" up -d --force-recreate zookeeper kafka; then
            echo "Kafka stack failed to start. Recent logs:" >&2
            docker compose -f "${COMPOSE_FILE}" logs --tail=200 zookeeper kafka || true
            exit 1
          fi

      - name: Health Checks
        shell: bash
        run: |
          set -euo pipefail

          DEPLOY_ROOT="${{ inputs.deploy_root }}"
          COMPOSE_FILE="${DEPLOY_ROOT}/kafka/docker-compose.yml"

          wait_container_healthy() {
            local name="$1"
            for _ in $(seq 1 40); do
              local status
              status="$(docker inspect -f '{{if .State.Health}}{{.State.Health.Status}}{{else}}{{.State.Status}}{{end}}' "$name" 2>/dev/null || true)"
              if [[ "${status}" == "healthy" ]]; then
                echo "${name} is healthy."
                return 0
              fi
              sleep 2
            done
            echo "${name} failed to become healthy." >&2
            docker compose -f "${COMPOSE_FILE}" logs --tail=200 "$name" || true
            return 1
          }

          wait_container_healthy "kafka-zookeeper-1"
          wait_container_healthy "kafka-kafka-1"

          # Ensure required topic exists after broker is healthy.
          docker exec kafka-kafka-1 \
            kafka-topics \
            --bootstrap-server 127.0.0.1:9092 \
            --create \
            --if-not-exists \
            --topic batchedUpdates \
            --partitions 1 \
            --replication-factor 1 >/dev/null

          if ! docker exec kafka-kafka-1 kafka-topics --bootstrap-server 127.0.0.1:9092 --list | grep -qx 'batchedUpdates'; then
            echo "Topic batchedUpdates missing after deploy." >&2
            docker exec kafka-kafka-1 kafka-topics --bootstrap-server 127.0.0.1:9092 --list || true
            exit 1
          fi

          echo "Kafka deployment succeeded."
